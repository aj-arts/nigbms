{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "\n",
    "primal = torch.randn(10, 10)\n",
    "tangent = torch.randn(10, 10)\n",
    "\n",
    "def fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "# All forward AD computation must be performed in the context of\n",
    "# a ``dual_level`` context. All dual tensors created in such a context\n",
    "# will have their tangents destroyed upon exit. This is to ensure that\n",
    "# if the output or intermediate results of this computation are reused\n",
    "# in a future forward AD computation, their tangents (which are associated\n",
    "# with this computation) won't be confused with tangents from the later\n",
    "# computation.\n",
    "with fwAD.dual_level():\n",
    "    # To create a dual tensor we associate a tensor, which we call the\n",
    "    # primal with another tensor of the same size, which we call the tangent.\n",
    "    # If the layout of the tangent is different from that of the primal,\n",
    "    # The values of the tangent are copied into a new tensor with the same\n",
    "    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n",
    "    #\n",
    "    # It is also important to note that the dual tensor created by\n",
    "    # ``make_dual`` is a view of the primal.\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    assert fwAD.unpack_dual(dual_input).tangent is tangent\n",
    "\n",
    "    # To demonstrate the case where the copy of the tangent happens,\n",
    "    # we pass in a tangent with a layout different from that of the primal\n",
    "    dual_input_alt = fwAD.make_dual(primal, tangent.T)\n",
    "    assert fwAD.unpack_dual(dual_input_alt).tangent is not tangent\n",
    "\n",
    "    # Tensors that do not have an associated tangent are automatically\n",
    "    # considered to have a zero-filled tangent of the same shape.\n",
    "    plain_tensor = torch.randn(10, 10)\n",
    "    dual_output = fn(dual_input, plain_tensor)\n",
    "\n",
    "    # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent``\n",
    "    # as attributes\n",
    "    jvp = fwAD.unpack_dual(dual_output).tangent\n",
    "\n",
    "assert fwAD.unpack_dual(dual_output).tangent is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict import TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "primal = TensorDict({'a': torch.arange(30).reshape(10, 3).float(), 'b': torch.ones(10, 4, 5)}, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangent = TensorDict({'a': torch.ones(10, 3), 'b': torch.ones(10, 4, 5)}, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dict(x):\n",
    "    return x['a'] ** 2 + x['b'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 200.,  201.,  204.],\n",
       "         [ 209.,  216.,  225.],\n",
       "         [ 236.,  249.,  264.],\n",
       "         [ 281.,  300.,  321.],\n",
       "         [ 344.,  369.,  396.],\n",
       "         [ 425.,  456.,  489.],\n",
       "         [ 524.,  561.,  600.],\n",
       "         [ 641.,  684.,  729.],\n",
       "         [ 776.,  825.,  876.],\n",
       "         [ 929.,  984., 1041.]]),\n",
       " tensor([[200., 202., 204.],\n",
       "         [206., 208., 210.],\n",
       "         [212., 214., 216.],\n",
       "         [218., 220., 222.],\n",
       "         [224., 226., 228.],\n",
       "         [230., 232., 234.],\n",
       "         [236., 238., 240.],\n",
       "         [242., 244., 246.],\n",
       "         [248., 250., 252.],\n",
       "         [254., 256., 258.]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jvp(f_dict, (primal,), (tangent,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'TensorDict' and 'TensorDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m tensordict2 \u001b[38;5;241m=\u001b[39m TensorDict({\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey1\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey2\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     13\u001b[0m }, batch_size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2つのtensordictを足す\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtensordict1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensordict2\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 結果を確認\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'TensorDict' and 'TensorDict'"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "import torch\n",
    "\n",
    "# 2つのtensordictを作成\n",
    "tensordict1 = TensorDict({\n",
    "    \"key1\": torch.randn(3, 4),\n",
    "    \"key2\": torch.randn(3, 4)\n",
    "}, batch_size=[3])\n",
    "\n",
    "tensordict2 = TensorDict({\n",
    "    \"key1\": torch.randn(3, 4),\n",
    "    \"key2\": torch.randn(3, 4)\n",
    "}, batch_size=[3])\n",
    "\n",
    "# 2つのtensordictを足す\n",
    "result = tensordict1 + tensordict2\n",
    "\n",
    "# 結果を確認\n",
    "print(result[\"key1\"])\n",
    "print(result[\"key2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'TensorDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensordict1\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'TensorDict'"
     ]
    }
   ],
   "source": [
    "1.0 * tensordict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensordict._td.TensorDict"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensordict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        key1: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        key2: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict1.apply(lambda x: 2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0484, -1.8561,  1.4272,  1.0533],\n",
       "        [-1.5890,  0.2027, -0.1566,  1.5146],\n",
       "        [ 1.2385,  2.0889,  1.0955, -1.2020]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict1.apply(lambda x: 2 * x)[\"key2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5242, -0.9280,  0.7136,  0.5267],\n",
       "        [-0.7945,  0.1013, -0.0783,  0.7573],\n",
       "        [ 0.6193,  1.0444,  0.5478, -0.6010]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict1['key2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tensordicts(tensordict1, tensordict2):\n",
    "    \"\"\"\n",
    "    2つのtensordictを足し合わせる関数\n",
    "    \n",
    "    Args:\n",
    "        tensordict1 (TensorDict): 1つ目のtensordict\n",
    "        tensordict2 (TensorDict): 2つ目のtensordict\n",
    "        \n",
    "    Returns:\n",
    "        TensorDict: 2つのtensordictを足し合わせた結果\n",
    "    \"\"\"\n",
    "    # 2つのtensordictのキーが同じであることを確認\n",
    "    assert set(tensordict1.keys()) == set(tensordict2.keys()), \"Keys in the two tensordicts must be the same.\"\n",
    "    \n",
    "    # 各keyに対応するtensorを足し合わせる\n",
    "    result = {k:tensordict1[k] + tensordict2[k] for k in tensordict1.keys()}\n",
    "    \n",
    "    # 新しいtensordictを作成して返す\n",
    "    return TensorDict(result, batch_size=tensordict1.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3702,  0.1352,  0.4325,  1.0031],\n",
       "        [-1.2659, -1.4851,  0.7063, -0.8183],\n",
       "        [ 0.7257,  0.3506,  0.5705, -0.2927]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_tensordicts(tensordict1, tensordict2)[\"key2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3702,  0.1352,  0.4325,  1.0031],\n",
       "        [-1.2659, -1.4851,  0.7063, -0.8183],\n",
       "        [ 0.7257,  0.3506,  0.5705, -0.2927]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict1[\"key2\"] + tensordict2[\"key2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(tensordict1, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nigbms.utils.solver import rademacher_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from petsc4py import PETSc\n",
    "from tensordict import TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
